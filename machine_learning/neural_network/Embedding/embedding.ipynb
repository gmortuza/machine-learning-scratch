{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "- What is Word Embedding?\n",
    "    - A technique to represent **words as dense vectors** in a lower-dimensional space.\n",
    "    - Captures **semantic relationships** between words based on their usage in a corpus.\n",
    "    - Similar words have close vector representations.\n",
    "        - E(King) - E(Man) + E(Woman) ≈ E(Queen)\n",
    "- Why Use Word Embeddings Instead of One-Hot Encoding?\n",
    "    - One-hot encoding is sparse and high-dimensional (e.g., for 50K words, it needs a 50K-dimensional vector).\n",
    "    - Word embeddings are dense and lower-dimensional (e.g., 100-300 dimensions), making them efficient.\n",
    "    - Embeddings capture meaning—words with similar meanings have closer vectors.\n",
    "    - One-hot encoding treats all words as independent, whereas embeddings learn relationships.\n",
    "- Common Word Embedding Models\n",
    "    - Word2Vec (CBOW & Skip-Gram) – Predicts words based on context.\n",
    "    - GloVe – Uses word co-occurrence matrices to find relationships.\n",
    "    - FastText – Embeds subword information, useful for rare words.\n",
    "    - Transformer-based Embeddings (BERT, GPT) – Contextual embeddings, meaning changes based on sentence structure.\n",
    "- How Word Embeddings are Learned?\n",
    "    - Word2Vec (CBOW & Skip-Gram): Predicts missing words in a context.\n",
    "    - Matrix Factorization (GloVe): Learns word relationships from co-occurrence statistics.\n",
    "    - Neural Networks (BERT, GPT): Uses deep learning models for contextual understanding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "import torch\n",
    "import torchtext\n",
    "import collections\n",
    "from collections import Counter\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constant\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
    "BATCH_SIZE = 128\n",
    "VOCAB_SIZE = 10_000\n",
    "EMBEDDING_DIM = 256\n",
    "TOTAL_EPOCH = 5\n",
    "MAX_TRAINING_EXAMPLE = 50_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset....\")\n",
    "train_iter = load_dataset(\"embedding-data/simple-wiki\", split='train')\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove punctuation\n",
    "    tokens = tokenizer(text) # tokenize words\n",
    "    \n",
    "    return tokens\n",
    "corpus = [preprocess_text(\".\".join(single_example[\"set\"])) for single_example in train_iter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 4515153\n",
      "Total unique words: 118134\n",
      "Vocabulary size:  10000\n"
     ]
    }
   ],
   "source": [
    "all_words = list(itertools.chain(*corpus))\n",
    "# get unique words\n",
    "word_frequncy = Counter(all_words)\n",
    "all_words_sorted_by_frequency = sorted(all_words, key=lambda x: word_frequncy[x])\n",
    "vocabulary = all_words_sorted_by_frequency[:VOCAB_SIZE - 1] # last one we will keep for unknown words\n",
    "\n",
    "unique_words = list(set(all_words))\n",
    "print(\"Total words:\", len(all_words))\n",
    "print(\"Total unique words:\", len(unique_words))\n",
    "print(\"Vocabulary size: \", len(vocabulary) + 1)\n",
    "\n",
    "# word to index mapper and index to word mapper\n",
    "word_to_index = {i: word for word, i in enumerate(vocabulary)}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Generate training data\n",
    "\n",
    "![alt text](https://i.sstatic.net/Urqj0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoB\n",
    "WINDOW_SIZE = 2\n",
    "\n",
    "def get_training_pairs(algorithm=\"cbow\", window_size=WINDOW_SIZE):\n",
    "    training_pairs = []\n",
    "\n",
    "    for sentence in corpus:\n",
    "        # convert the word into index\n",
    "        sentence_with_index = [word_to_index.get(word, VOCAB_SIZE -  1) for word in sentence]        \n",
    "        # Algorithm for Continious bag of words\n",
    "        # ---?\n",
    "        if algorithm == \"cbow\":\n",
    "            for index in range(window_size, len(sentence_with_index)):\n",
    "                training_pair = sentence_with_index[index-window_size:index+1]\n",
    "                training_pairs.append(training_pair)\n",
    "        else: # skip_gram\n",
    "            # Algorithm for skip_gram\n",
    "            # ??_??\n",
    "            for index in range(window_size, len(sentence_with_index) - window_size):\n",
    "                training_pair = sentence_with_index[index-window_size : index] + sentence_with_index[index + 1 : index + window_size+1] + [sentence_with_index[index]]\n",
    "                training_pairs.append(training_pair)\n",
    "            \n",
    "    return training_pairs\n",
    "\n",
    "training_pairs = get_training_pairs()\n",
    "# Convert training pairs to tensors\n",
    "train_data = torch.tensor(training_pairs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data[:MAX_TRAINING_EXAMPLE]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index][:-1], self.data[index][-1]\n",
    "    \n",
    "\n",
    "training_dataset = TextDataset(train_data)\n",
    "    \n",
    "train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Define word embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding layer is just linear layer with efficient mat multiplications and without bias term\n",
    "- Why embedding layer doesn't uses bias terms:\n",
    "    - Any bias term would apply **the same shift to all words**, which isn’t useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        # Instead of an embedding layer, we could use a Linear layer without the bias term\n",
    "        # However, using a Linear layer requires inputting one-hot encoded vectors, \n",
    "        # which are sparse and inefficient for large vocabularies.\n",
    "        # The embedding layer directly maps word indices to dense vectors, making it memory-efficient.\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # The output layer projects the embeddings to a probability distribution over the vocabulary.\n",
    "        # This is similar to a softmax layer in a classification model.\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # The embedding layer returns a tensor of shape (batch_size, window, embedding_dim).\n",
    "        # We take the mean along the context dimension to get a single vector per batch element.\n",
    "        embed = self.embedding_layer(x).mean(dim=1)\n",
    "        output = self.output_layer(embed)\n",
    "        return output\n",
    "\n",
    "model = Word2Vec(VOCAB_SIZE, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "total_loss = 0.0\n",
    "# \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "for epoch in range(TOTAL_EPOCH):\n",
    "    epoch_loss = 0.0\n",
    "    total_batch = 0\n",
    "    with tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\", leave=False) as tbar:\n",
    "        for inputs_batch, label_batch in tbar:\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(inputs_batch)\n",
    "            loss = loss_fn(predictions, label_batch)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            total_batch += 1\n",
    "            tbar.set_postfix(loss=epoch_loss/total_batch)\n",
    "    # print(\"Epoch: \", epoch + 1, \"/\", TOTAL_EPOCH)\n",
    "    # print(\"\\t\\t Loss: \", round(total_loss, 2))\n",
    "\n",
    "\n",
    "# Return average loss\n",
    "# return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Find similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given word: comforts\n",
      "Similar words:  naujamiestis, halogen, takur, saville, orientalis\n"
     ]
    }
   ],
   "source": [
    "def find_most_similar_words(model, word, word_to_index, top_k = 5):\n",
    "    if word not in word_to_index: \n",
    "        print(\"Word not found in the vocabulary\")\n",
    "        return \n",
    "    # step 1: Get the index of the word from the vocbulary. That index is also the index in the embedding layer\n",
    "    word_index = torch.tensor([word_to_index[word]], dtype=torch.long)\n",
    "    # step 2: Get the embeddings of that word index from the embedding layers\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        word_embedding = model.embedding_layer(word_index) # shape: (1, EMBEDDING_DIM)\n",
    "        word_embedding = word_embedding.squeeze(0) # shape(EMBEDDING_DIM, )\n",
    "    \n",
    "    # step 3: Get all the embedding from the model\n",
    "    all_embeddings = model.embedding_layer.weight # shape (Vocab_size, embedding_dim)\n",
    "\n",
    "    # step 4: Find cosine similarity of the given words with all embeddings\n",
    "    similarities = F.cosine_similarity(word_embedding, all_embeddings) # shape: (Vocab_size, )\n",
    "    # Get top k words\n",
    "    similar_indices = torch.argsort(similarities, descending=True).tolist()\n",
    "    # remove the given word\n",
    "    similar_indices.remove(word_index.item())\n",
    "    # Get top K word:\n",
    "    similar_words = [index_to_word[index] for index in similar_indices[:top_k]]\n",
    "    similar_words_txt = \", \".join(similar_words)\n",
    "    print(\"Given word:\", word)\n",
    "    print(\"Similar words: \", similar_words_txt)\n",
    "\n",
    "find_most_similar_words(model, \"comforts\", word_to_index, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
